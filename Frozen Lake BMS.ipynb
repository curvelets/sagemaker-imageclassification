{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Playing Frozen Lake with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this tutorial, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms. Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Letâ€™s start building our Q-table algorithm, which will try to solve Frozen Lake environment. In this environment aim is to reach the goal, on a frozen lake that might have some holes in it. Here is how surface is depicted by this algorithm.\n",
    "\n",
    "\n",
    "\n",
    "    SFFF       (S: starting point, safe) \n",
    "    FHFH       (F: frozen surface, safe)\n",
    "    FFFH       (H: hole, fall to your doom)\n",
    "    HFFG       (G: goal, where the frisbee is located)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1- Install gym library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2- Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3- Load Gym environment for Frozen Lake game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4- Initialize Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**<span style=\"color:blue\">Initializing a q-table means that we need to create a 2D matrix (array), set the right dimensions and all cells set to 0. This can be done using \"numpy\" library- specifically lookup nump.zeros() and you will be on your way for solving it! Q table needs to have its number of rows equal to \"state_space_size\" parameter and its columns need to be equal to \"action_space_size\".</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "#               Your code for initializing q table goes here\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "######################################################################\n",
    "\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5- Set Q-learning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**<span style=\"color:blue\">\"num_episodes\" is the maximum number of times (episodes) we want to play the game to learn it while training. There needs to be enough number of times so that the agent has enough time to learn how to play. For this example one hundred thousands episodes is required to make sure the agent can learn how to play the game. </span>**\n",
    "    \n",
    "**<span style=\"color:blue\">\"max_steps_per_episode\" is the number steps the agent can take each time it plays the game, to get to the goal, without failing. We should not choose a very large value for this because we want the agent to learn to get to the goal quickly. For this example, we can try a few hundreds of steps per episode maximum. </span>**\n",
    "\n",
    "**<span style=\"color:blue\">\"learning_rate\", is a parameter that defines how fast we want to update the Q-table. It is usually a small number at the lower range of [0, 1] interval. The lower the learning rate is, the slower Q-table will be updated and vice versa. </span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_episodes = 100000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.01\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = .1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.01\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 6- Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$Q^{New}_{(s,a)} = Q_{(s,a)} + \\alpha [\\Delta Q_{(s,a)}]$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\Delta Q_{(s,a)} =  R_{(s,a)} + \\gamma\\max_a Q'_{(s',a')} - Q_{(s,a)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Bellman equation can also be written in the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$Q^{New}_{(s,a)} = Q_{(s,a)}(1 - \\alpha) + \\alpha [R_{(s,a)} + \\gamma\\max_a Q'_{(s',a')}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "where:\n",
    "    \n",
    "$$Q^{New}_{(s,a)} : new \\ Q \\ value$$\n",
    "$$Q_{(s,a)} : current \\ Q \\ value$$   \n",
    "$$ \\alpha : learning \\ rate$$ \n",
    "$$R_{(s,a)} : reward \\ for \\  taking \\  action\\  a \\ at\\  state\\  s $$\n",
    "$$\\gamma : discount \\ rate $$\n",
    "$$\\max_a Q'_{(s',a')} :  maximum \\ expected \\ future \\ reward$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def Bellman(q_table1, action1, state1, learning_rate1, reward1, discount_rate1,new_state1):\n",
    "    \n",
    "    Q_sa_new = q_table1[state1, action1] * (1 - learning_rate1) + learning_rate1 *\\\n",
    "    (reward1 + discount_rate1 * np.max(q_table1[new_state1, :]))\n",
    "    return Q_sa_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 7- Exploration decay equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ \\epsilon  = \\epsilon_{min} + (\\epsilon_{max} - \\epsilon_{min}) \\exp(- d_{\\epsilon} * episode)$$\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "$$\\epsilon : exploration \\ rate$$ \n",
    "$$\\epsilon_{min} : min \\ exploration \\ rate$$ \n",
    "$$\\epsilon_{max} : max \\ exploration \\ rate$$ \n",
    "$$d_{\\epsilon} : exploration\\ decay \\ rate $$\n",
    "$$episode : episode \\ number$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def Exploration_decay(min_exploration_rate1, max_exploration_rate1, exploration_decay_rate1, episode1):\n",
    "    \n",
    "    exploration_rate1 = min_exploration_rate1 + \\\n",
    "    (max_exploration_rate1 - min_exploration_rate1) * np.exp(-exploration_decay_rate1*episode1)\n",
    "    return exploration_rate1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 8- Train/ Update Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "        # Update Q-table for Q(s,a)\n",
    "        qsa = Bellman(q_table, action, state, learning_rate, reward, discount_rate,new_state)\n",
    "        q_table[state, action] = qsa\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "        if done == True: \n",
    "            break\n",
    "    # Exploration rate decay\n",
    "    exploration_rate = Exploration_decay(min_exploration_rate, max_exploration_rate, exploration_decay_rate, episode)\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 9- Calculate and print the average reward per thousand episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 10- Check your updated Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 11- Let it play the game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        env.close()\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
